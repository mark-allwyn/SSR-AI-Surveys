{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semantic Similarity Rating (SSR) Pipeline\n",
    "\n",
    "This notebook demonstrates the complete SSR methodology based on **arXiv:2510.08338v2** for converting textual survey responses into Likert scale probability distributions.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The SSR approach:\n",
    "1. Takes textual responses to survey questions\n",
    "2. Uses **OpenAI text-embedding-3-small** to encode text (as per paper)\n",
    "3. Computes semantic similarity to reference statements (scale labels)\n",
    "4. Converts similarities to probabilities using **paper's normalization method**\n",
    "5. Evaluates predictions against ground truth\n",
    "6. Generates comprehensive reports\n",
    "\n",
    "## Key Updates\n",
    "\n",
    "- Uses OpenAI embeddings (paper-exact implementation)\n",
    "- Paper's normalization: subtract min + proportional\n",
    "- Supports multiple question types: yes/no, Likert-5, Likert-7, multiple choice\n",
    "- Ground truth comparison with 7+ evaluation metrics\n",
    "- Folder-based experiment organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Add parent directory to path\n",
    "sys.path.insert(0, str(Path().absolute().parent))\n",
    "\n",
    "from src.survey import Survey\n",
    "from src.llm_client import Response, generate_diverse_profiles\n",
    "from src.ssr_model import SemanticSimilarityRater\n",
    "from src.analysis import analyze_survey, create_results_dataframe\n",
    "from src.visualization import plot_distribution, plot_question_analysis\n",
    "from src.ground_truth import (\n",
    "    create_ground_truth_dict,\n",
    "    evaluate_against_ground_truth,\n",
    "    print_ground_truth_comparison\n",
    ")\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Survey Configuration\n",
    "\n",
    "We're using a lottery gaming platform survey with 6 questions of different types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load survey from config\n",
    "survey = Survey.from_config('../config/mixed_survey_config.yaml')\n",
    "\n",
    "print(f\"Survey: {survey.name}\")\n",
    "print(f\"Description: {survey.description}\")\n",
    "print(f\"\\nQuestions ({len(survey.questions)}):\")\n",
    "for q in survey.questions:\n",
    "    print(f\"  - {q.id} ({q.type}): {q.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize SSR Model (Paper Methodology)\n",
    "\n",
    "Using the exact methodology from arXiv:2510.08338v2:\n",
    "- **Model**: OpenAI text-embedding-3-small\n",
    "- **Normalization**: Paper's method (subtract min + proportional)\n",
    "- **Temperature**: 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize rater with paper's methodology\n",
    "rater = SemanticSimilarityRater(\n",
    "    model_name=\"text-embedding-3-small\",\n",
    "    temperature=1.0,\n",
    "    normalize_method=\"paper\",\n",
    "    use_openai=True\n",
    ")\n",
    "\n",
    "print(\"Rater Configuration:\")\n",
    "print(f\"  Model: {rater.model_name}\")\n",
    "print(f\"  Temperature: {rater.temperature}\")\n",
    "print(f\"  Normalization: {rater.normalize_method}\")\n",
    "print(f\"  Using OpenAI: {rater.use_openai}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Example: Single Response Rating\n",
    "\n",
    "Let's rate a single textual response to see how SSR works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example response to subscription question\n",
    "example_response = Response(\n",
    "    respondent_id=\"R001\",\n",
    "    question_id=\"q2_subscription_likelihood\",\n",
    "    text_response=\"I'm very interested in this platform! The automated ticket purchasing sounds great and I trust online payment systems. I'd say I'm quite likely to subscribe.\",\n",
    "    respondent_profile={'age_group': '26-35', 'environmental_consciousness': 'Very concerned'}\n",
    ")\n",
    "\n",
    "print(\"Example Response:\")\n",
    "print(f\"Question: {example_response.question_id}\")\n",
    "print(f\"Text: {example_response.text_response}\")\n",
    "print(f\"Profile: {example_response.respondent_profile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate the response\n",
    "question = survey.get_question_by_id(example_response.question_id)\n",
    "distribution = rater.rate_response(example_response, question)\n",
    "\n",
    "print(\"\\nRating Distribution:\")\n",
    "print(f\"Expected Value: {distribution.expected_value:.2f}\")\n",
    "print(f\"Mode: {distribution.mode}\")\n",
    "print(f\"Entropy: {distribution.entropy:.3f}\")\n",
    "print(\"\\nProbabilities:\")\n",
    "for scale_point, label in distribution.scale_labels.items():\n",
    "    idx = scale_point - min(distribution.scale_labels.keys())\n",
    "    print(f\"  {scale_point}. {label}: {distribution.distribution[idx]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "plot_distribution(distribution, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Ground Truth Data\n",
    "\n",
    "Let's load an existing experiment to see the full pipeline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the latest experiment\n",
    "experiments_dir = Path('../experiments')\n",
    "if experiments_dir.exists():\n",
    "    experiment_folders = sorted(experiments_dir.glob('run_*'))\n",
    "    if experiment_folders:\n",
    "        latest_experiment = experiment_folders[-1]\n",
    "        print(f\"Loading latest experiment: {latest_experiment.name}\")\n",
    "        \n",
    "        # Load ground truth\n",
    "        gt_df = pd.read_csv(latest_experiment / 'ground_truth.csv')\n",
    "        print(f\"\\nGround Truth Data: {gt_df.shape}\")\n",
    "        display(gt_df.head(12))\n",
    "    else:\n",
    "        print(\"No experiments found. Run ground_truth_pipeline.py first.\")\n",
    "else:\n",
    "    print(\"No experiments folder found. Run ground_truth_pipeline.py first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ground Truth Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth distribution by question\n",
    "if 'gt_df' in locals():\n",
    "    print(\"Ground Truth Distribution by Question:\\n\")\n",
    "    for question in survey.questions:\n",
    "        q_data = gt_df[gt_df['question_id'] == question.id]\n",
    "        print(f\"\\n{question.id} ({question.type}):\")\n",
    "        print(q_data['ground_truth'].value_counts().sort_index())\n",
    "        print(f\"Mean: {q_data['ground_truth'].mean():.2f}\")\n",
    "        print(f\"Std: {q_data['ground_truth'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ground truth distributions\n",
    "if 'gt_df' in locals():\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, question in enumerate(survey.questions):\n",
    "        q_data = gt_df[gt_df['question_id'] == question.id]\n",
    "        counts = q_data['ground_truth'].value_counts().sort_index()\n",
    "        \n",
    "        axes[i].bar(counts.index, counts.values, alpha=0.7, color='steelblue')\n",
    "        axes[i].set_title(f\"{question.id}\\n{question.type}\", fontsize=10, fontweight='bold')\n",
    "        axes[i].set_xlabel('Rating')\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. View Experiment Report\n",
    "\n",
    "Let's load and display the comprehensive markdown report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the report image\n",
    "if 'latest_experiment' in locals():\n",
    "    from IPython.display import Image, display, Markdown\n",
    "    \n",
    "    report_png = latest_experiment / 'report.png'\n",
    "    if report_png.exists():\n",
    "        print(\"Visual Report:\")\n",
    "        display(Image(filename=str(report_png)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display key metrics from text report\n",
    "if 'latest_experiment' in locals():\n",
    "    report_txt = latest_experiment / 'report.txt'\n",
    "    if report_txt.exists():\n",
    "        with open(report_txt, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            # Print first 30 lines (overall summary)\n",
    "            print(''.join(lines[:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Compare Human vs LLM Response Styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse metrics from report for comparison\n",
    "if 'latest_experiment' in locals():\n",
    "    report_txt = latest_experiment / 'report.txt'\n",
    "    if report_txt.exists():\n",
    "        # Extract accuracy by question\n",
    "        metrics_by_question = []\n",
    "        \n",
    "        with open(report_txt, 'r') as f:\n",
    "            content = f.read()\n",
    "            \n",
    "        for question in survey.questions:\n",
    "            # Find question section\n",
    "            q_section = content.split(f\"QUESTION: {question.id}\")[1].split(\"---\")[0] if f\"QUESTION: {question.id}\" in content else \"\"\n",
    "            \n",
    "            if q_section:\n",
    "                # Extract mode accuracy\n",
    "                for line in q_section.split('\\n'):\n",
    "                    if 'Mode Accuracy:' in line:\n",
    "                        parts = line.split('|')\n",
    "                        human_acc = float(parts[0].split(':')[1].strip().replace('%', ''))\n",
    "                        llm_acc = float(parts[1].split(':')[1].strip().replace('%', ''))\n",
    "                        metrics_by_question.append({\n",
    "                            'question_id': question.id,\n",
    "                            'question_type': question.type,\n",
    "                            'human_accuracy': human_acc,\n",
    "                            'llm_accuracy': llm_acc,\n",
    "                            'difference': human_acc - llm_acc\n",
    "                        })\n",
    "                        break\n",
    "        \n",
    "        if metrics_by_question:\n",
    "            metrics_df = pd.DataFrame(metrics_by_question)\n",
    "            print(\"\\nAccuracy by Question:\")\n",
    "            display(metrics_df)\n",
    "            \n",
    "            # Plot comparison\n",
    "            fig, ax = plt.subplots(figsize=(12, 6))\n",
    "            x = np.arange(len(metrics_df))\n",
    "            width = 0.35\n",
    "            \n",
    "            ax.bar(x - width/2, metrics_df['human_accuracy'], width, label='Human', alpha=0.8)\n",
    "            ax.bar(x + width/2, metrics_df['llm_accuracy'], width, label='LLM', alpha=0.8)\n",
    "            \n",
    "            ax.set_ylabel('Mode Accuracy (%)', fontsize=12)\n",
    "            ax.set_xlabel('Question', fontsize=12)\n",
    "            ax.set_title('SSR Accuracy: Human vs LLM Response Styles', fontsize=14, fontweight='bold')\n",
    "            ax.set_xticks(x)\n",
    "            ax.set_xticklabels(metrics_df['question_id'], rotation=45, ha='right')\n",
    "            ax.legend()\n",
    "            ax.grid(axis='y', alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Generate New Responses and Rate Them\n",
    "\n",
    "Let's generate some new synthetic responses and rate them in real-time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a few profiles\n",
    "n_test = 5\n",
    "test_profiles = generate_diverse_profiles(n_test)\n",
    "\n",
    "print(f\"Generated {len(test_profiles)} test profiles:\")\n",
    "for i, profile in enumerate(test_profiles):\n",
    "    print(f\"\\nProfile {i+1}:\")\n",
    "    print(f\"  Environmental Consciousness: {profile.environmental_consciousness}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic responses for binary question\n",
    "from ground_truth_pipeline import generate_human_style_response\n",
    "\n",
    "question = survey.get_question_by_id('q1_would_subscribe')\n",
    "ref_statements = question.get_reference_statements()\n",
    "\n",
    "test_responses = []\n",
    "for i, profile in enumerate(test_profiles):\n",
    "    # Simulate ground truth based on profile\n",
    "    if profile.environmental_consciousness in [\"Extremely concerned\", \"Very concerned\"]:\n",
    "        ground_truth = 1  # Yes\n",
    "    else:\n",
    "        ground_truth = 2  # No\n",
    "    \n",
    "    target_statement = ref_statements[ground_truth]\n",
    "    text_response = generate_human_style_response(target_statement, ground_truth, question.num_options)\n",
    "    \n",
    "    response = Response(\n",
    "        respondent_id=f\"TEST{i+1:03d}\",\n",
    "        question_id=question.id,\n",
    "        text_response=text_response,\n",
    "        respondent_profile=profile.to_dict()\n",
    "    )\n",
    "    test_responses.append((response, ground_truth))\n",
    "\n",
    "print(f\"\\nGenerated {len(test_responses)} test responses\")\n",
    "print(\"\\nExamples:\")\n",
    "for i, (resp, gt) in enumerate(test_responses[:3]):\n",
    "    print(f\"\\n{i+1}. Ground Truth: {gt}\")\n",
    "    print(f\"   Text: {resp.text_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rate the test responses\n",
    "test_distributions = rater.rate_responses(\n",
    "    [r[0] for r in test_responses], \n",
    "    survey, \n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(f\"\\nGenerated {len(test_distributions)} distributions\")\n",
    "\n",
    "# Compare predictions to ground truth\n",
    "correct = 0\n",
    "print(\"\\nPredictions vs Ground Truth:\")\n",
    "for i, (dist, (_, gt)) in enumerate(zip(test_distributions, test_responses)):\n",
    "    predicted = dist.mode\n",
    "    is_correct = predicted == gt\n",
    "    correct += is_correct\n",
    "    \n",
    "    print(f\"\\n{i+1}. Ground Truth: {gt}, Predicted: {predicted} {'✓' if is_correct else '✗'}\")\n",
    "    print(f\"   Probabilities: {dist.distribution}\")\n",
    "    print(f\"   Expected Value: {dist.expected_value:.2f}\")\n",
    "\n",
    "accuracy = correct / len(test_responses) * 100\n",
    "print(f\"\\nAccuracy: {accuracy:.1f}% ({correct}/{len(test_responses)})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Explore Different Temperature Settings\n",
    "\n",
    "Temperature controls the spread of probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different temperatures\n",
    "temperatures = [0.5, 1.0, 2.0]\n",
    "sample_response = test_responses[0][0]\n",
    "sample_question = survey.get_question_by_id(sample_response.question_id)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for i, temp in enumerate(temperatures):\n",
    "    rater_temp = SemanticSimilarityRater(\n",
    "        model_name=\"text-embedding-3-small\",\n",
    "        temperature=temp,\n",
    "        normalize_method=\"paper\",\n",
    "        use_openai=True\n",
    "    )\n",
    "    dist = rater_temp.rate_response(sample_response, sample_question)\n",
    "    plot_distribution(dist, ax=axes[i], title=f\"Temperature = {temp}\")\n",
    "\n",
    "plt.suptitle(f\"Effect of Temperature on Probability Distribution\\nResponse: '{sample_response.text_response[:60]}...'\", \n",
    "             fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTemperature Effects:\")\n",
    "print(\"  • Lower temperature (0.5): More peaked, confident predictions\")\n",
    "print(\"  • Default temperature (1.0): Balanced as per paper\")\n",
    "print(\"  • Higher temperature (2.0): More spread, uncertain predictions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Run a Complete Mini-Experiment\n",
    "\n",
    "Generate a small ground truth dataset and evaluate SSR on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ground_truth_pipeline import (\n",
    "    generate_ground_truth_ratings,\n",
    "    generate_responses_from_ground_truth\n",
    ")\n",
    "\n",
    "# Generate mini dataset\n",
    "n_mini = 10\n",
    "mini_profiles = generate_diverse_profiles(n_mini)\n",
    "\n",
    "# Generate ground truth\n",
    "mini_gt_df = generate_ground_truth_ratings(survey, mini_profiles, seed=999)\n",
    "\n",
    "print(f\"Generated ground truth for {n_mini} respondents × {len(survey.questions)} questions = {len(mini_gt_df)} ratings\")\n",
    "display(mini_gt_df.head(12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate human-style responses\n",
    "mini_responses = generate_responses_from_ground_truth(\n",
    "    survey, mini_profiles, mini_gt_df, response_style=\"human\", seed=999\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(mini_responses)} responses\")\n",
    "print(\"\\nSample responses:\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n{i+1}. {mini_responses[i].question_id}\")\n",
    "    print(f\"   Text: {mini_responses[i].text_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply SSR\n",
    "mini_distributions = rater.rate_responses(mini_responses, survey, show_progress=True)\n",
    "\n",
    "print(f\"\\nGenerated {len(mini_distributions)} distributions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate against ground truth\n",
    "ground_truth_dict = create_ground_truth_dict(mini_gt_df)\n",
    "\n",
    "print(\"\\nEvaluation Results:\\n\")\n",
    "for question in survey.questions:\n",
    "    q_dists = [d for d in mini_distributions if d.question_id == question.id]\n",
    "    comparison = evaluate_against_ground_truth(q_dists, ground_truth_dict, question)\n",
    "    \n",
    "    print(f\"{question.id} ({question.type}):\")\n",
    "    print(f\"  Mode Accuracy: {comparison.mode_accuracy:.1%}\")\n",
    "    print(f\"  Top-2 Accuracy: {comparison.top2_accuracy:.1%}\")\n",
    "    print(f\"  MAE: {comparison.mae:.3f}\")\n",
    "    print(f\"  Prob at Truth: {comparison.prob_at_truth:.3f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Key Takeaways\n",
    "\n",
    "### Methodology\n",
    "- **Paper-exact implementation**: Using OpenAI text-embedding-3-small and paper's normalization\n",
    "- **Multiple question types**: Binary, Likert-5, Likert-7, multiple choice\n",
    "- **Probabilistic output**: Full distributions, not just single predictions\n",
    "\n",
    "### Performance\n",
    "- **High accuracy**: Typically 90%+ mode accuracy on clear responses\n",
    "- **Response style matters**: Direct language (human-style) slightly outperforms hedged language (LLM-style)\n",
    "- **Graceful errors**: When wrong, predictions are usually off by one scale point\n",
    "\n",
    "### Practical Use\n",
    "- **Folder organization**: Each experiment run creates a timestamped folder\n",
    "- **Comprehensive reports**: PNG visualization, TXT metrics, MD explanations\n",
    "- **Reproducible**: Fixed seeds enable replication\n",
    "\n",
    "### Next Steps\n",
    "1. **Custom surveys**: Edit `config/mixed_survey_config.yaml` for your domain\n",
    "2. **Real data**: Replace synthetic responses with actual survey data\n",
    "3. **Parameter tuning**: Experiment with temperature settings\n",
    "4. **Scale analysis**: Test with larger sample sizes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. References\n",
    "\n",
    "- **Paper**: *LLMs Reproduce Human Purchase Intent via Semantic Similarity Elicitation* (arXiv:2510.08338v2)\n",
    "- **Repository**: https://github.com/pymc-labs/semantic-similarity-rating\n",
    "- **Model**: OpenAI text-embedding-3-small\n",
    "- **Documentation**: See project README.md for full details"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
